#!/usr/bin/env bash
# Run Performance Benchmarks Script
# Executes performance benchmarks and collects metrics

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"
cd "${REPO_ROOT}"

# Parse arguments
OUTPUT_DIR="${ARTIFACTS_PATH:-${REPO_ROOT}/tests/artifacts/perf}"
while [[ $# -gt 0 ]]; do
    case $1 in
        --out)
            OUTPUT_DIR="$2"
            shift 2
            ;;
        *)
            echo "Unknown option: $1"
            echo "Usage: $0 [--out OUTPUT_DIR]"
            exit 1
            ;;
    esac
done

mkdir -p "${OUTPUT_DIR}"

# Colors for output
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

print_status() {
    echo -e "${GREEN}âœ“${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}âš ${NC} $1"
}

echo "ðŸ“Š Running performance benchmarks..."

# 1. Kernel Benchmarks
if [ -f "${REPO_ROOT}/target/x86_64-aios/release/benchmark" ]; then
    print_status "Running kernel benchmarks..."
    "${REPO_ROOT}/target/x86_64-aios/release/benchmark" \
        --output "${OUTPUT_DIR}/kernel_benchmarks.json" 2>&1 | tee "${OUTPUT_DIR}/kernel_benchmarks.log" || {
        print_warning "Kernel benchmarks failed"
    }
else
    print_warning "Kernel benchmark binary not found"
fi

# 2. Run Performance Test Suite (if test runner exists)
if [ -f "${REPO_ROOT}/target/release/aios-test-runner" ]; then
    print_status "Running performance test suite..."
    "${REPO_ROOT}/target/release/aios-test-runner" \
        performance \
        --output "${OUTPUT_DIR}" \
        --report "${OUTPUT_DIR}/summary.json" \
        --verbose 2>&1 | tee "${OUTPUT_DIR}/performance.log" || {
        print_warning "Some performance tests may have failed"
    }
elif [ -f "${REPO_ROOT}/tests/integration/target/release/aios-test-runner" ]; then
    print_status "Running performance test suite (from tests/integration)..."
    "${REPO_ROOT}/tests/integration/target/release/aios-test-runner" \
        performance \
        --output "${OUTPUT_DIR}" \
        --report "${OUTPUT_DIR}/summary.json" \
        --verbose 2>&1 | tee "${OUTPUT_DIR}/performance.log" || {
        print_warning "Some performance tests may have failed"
    }
else
    print_warning "Performance test runner not found, creating placeholder metrics"
    # Fallback to placeholder JSON
    cat > "${OUTPUT_DIR}/latency.json" <<EOF
{
  "workload": "latency",
  "agents": 100,
  "duration_seconds": 60,
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "metrics": {
    "avg_latency_ms": 0,
    "p50_latency_ms": 0,
    "p95_latency_ms": 0,
    "p99_latency_ms": 0,
    "status": "placeholder"
  }
}
EOF
fi

# 5. Memory Benchmarks (if available)
if [ -f "${REPO_ROOT}/target/release/kernel_profiler" ]; then
    print_status "Running memory benchmarks..."
    "${REPO_ROOT}/target/release/kernel_profiler" \
        --memory \
        --output "${OUTPUT_DIR}/memory.json" 2>&1 | tee "${OUTPUT_DIR}/memory.log" || {
        print_warning "Memory benchmarks failed"
    }
fi

# 6. Generate Summary
cat > "${OUTPUT_DIR}/summary.json" <<EOF
{
  "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "output_dir": "${OUTPUT_DIR}",
  "benchmarks": {
    "latency": "$([ -f "${OUTPUT_DIR}/latency.json" ] && echo "completed" || echo "failed")",
    "throughput": "$([ -f "${OUTPUT_DIR}/throughput.json" ] && echo "completed" || echo "failed")",
    "mixed": "$([ -f "${OUTPUT_DIR}/mixed.json" ] && echo "completed" || echo "failed")",
    "kernel": "$([ -f "${OUTPUT_DIR}/kernel_benchmarks.json" ] && echo "completed" || echo "skipped")"
  }
}
EOF

print_status "Performance benchmarks completed"
echo "Results saved to: ${OUTPUT_DIR}"
